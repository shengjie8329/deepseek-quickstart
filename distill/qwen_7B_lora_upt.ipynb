{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ Unsloth å¯¹ DeepSeek-R1-Distill-Qwen-1.5B æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒ\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `unsloth` åº“å¯¹ `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„ QLoRA (Low-Rank Adaptation) å¾®è°ƒã€‚\n",
    "\n",
    "æ•´ä¸ªæµç¨‹åŒ…æ‹¬ï¼š\n",
    "1.  ç¯å¢ƒå‡†å¤‡ä¸åº“å¯¼å…¥\n",
    "2.  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)ã€‚\n",
    "3.  åœ¨å¾®è°ƒå‰ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç®€å•çš„æ¨ç†æµ‹è¯•ã€‚\n",
    "4.  ä¸‹è½½å’Œæ ¼å¼åŒ–è®­ç»ƒæ•°æ®é›†\n",
    "5.  ä½¿ç”¨ `unsloth` çš„ `FastLanguageModel` æ¥ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ã€‚\n",
    "6.  é…ç½® `SFTTrainer` ç›‘ç£å¾®è°ƒè®­ç»ƒé…ç½®ã€‚\n",
    "7.  å¯åŠ¨è®­ç»ƒï¼Œå¹¶è§‚å¯Ÿ Loss å˜åŒ–æƒ…å†µ\n",
    "8.  ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
    "9.  æµ‹è¯•è®­ç»ƒåçš„ç”Ÿæˆç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ç¯å¢ƒå‡†å¤‡ä¸åº“å¯¼å…¥\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å¹¶å¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“ã€‚`transformers` ç”¨äºåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œ`unsloth` ç”¨äºé«˜æ•ˆå¾®è°ƒï¼Œ`trl` æä¾›äº† `SFTTrainer`ï¼Œè€Œ `datasets` ç”¨äºå¤„ç†æ•°æ®ã€‚\n",
    "\n",
    "**æ³¨æ„**: åœ¨è¿è¡Œæ­¤ Notebook ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…ï¼š\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä¸‹è½½å™¨å°±ä¼šä½¿ç”¨é•œåƒç«™\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: datasets==3.6.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: pandas==2.3.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.3.1)\n",
      "Requirement already satisfied: peft==0.17.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.17.0)\n",
      "Requirement already satisfied: timm==1.0.19 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.0.19)\n",
      "Requirement already satisfied: torch==2.7.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.7.1)\n",
      "Requirement already satisfied: torchvision==0.22.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.22.1)\n",
      "Requirement already satisfied: transformers==4.55.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (4.55.2)\n",
      "Requirement already satisfied: trl==0.21.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: unsloth==2025.8.5 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2025.8.5)\n",
      "Requirement already satisfied: unsloth_zoo==2025.8.4 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2025.8.4)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (0.34.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from datasets==3.6.0->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas==2.3.1->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from peft==0.17.0->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from peft==0.17.0->-r requirements.txt (line 3)) (1.10.1)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from peft==0.17.0->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torch==2.7.1->-r requirements.txt (line 5)) (3.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from torchvision==0.22.1->-r requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from transformers==4.55.2->-r requirements.txt (line 7)) (2025.7.33)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from transformers==4.55.2->-r requirements.txt (line 7)) (0.21.4)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.0.31.post1)\n",
      "Requirement already satisfied: bitsandbytes in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.47.0)\n",
      "Requirement already satisfied: tyro in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.9.28)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.45.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (6.32.0)\n",
      "Requirement already satisfied: hf_transfer in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth==2025.8.5->-r requirements.txt (line 9)) (0.35.1)\n",
      "Requirement already satisfied: cut_cross_entropy in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth_zoo==2025.8.4->-r requirements.txt (line 10)) (25.1.1)\n",
      "Requirement already satisfied: msgspec in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from unsloth_zoo==2025.8.4->-r requirements.txt (line 10)) (0.19.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from triton==3.3.1->torch==2.7.1->-r requirements.txt (line 5)) (75.8.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets==3.6.0->-r requirements.txt (line 1)) (1.1.9)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.3.1->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from requests>=2.32.2->datasets==3.6.0->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.7.1->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: importlib_metadata in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from diffusers->unsloth==2025.8.5->-r requirements.txt (line 9)) (8.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from jinja2->torch==2.7.1->-r requirements.txt (line 5)) (3.0.2)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (14.1.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (1.7.2)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (4.4.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0->-r requirements.txt (line 1)) (1.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from importlib_metadata->diffusers->unsloth==2025.8.5->-r requirements.txt (line 9)) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth==2025.8.5->-r requirements.txt (line 9)) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt --trusted-host mirrors.aliyun.com -i http://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Qwen2 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.545 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75efee68f1244d198184afacc0983e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80205c523c14bc68682d5b211d43379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811869730eac455891b876883697e368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59662cac8a80455db3491ea754aadae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01509ab5cc264fac8c91c7800ecb76b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f801ebe15ca47bd99dc98a5f870ed06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671194a39c164ab2bab9fba05fa6df35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cf55cc703d4d66bd3725f4e2984943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# å®šä¹‰æ¨¡å‹å’Œä¸€äº›åŸºæœ¬å‚æ•°\n",
    "max_seq_length = 8192\n",
    "dtype = None # None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹© (Float16 a T4, V100, BFloat16 a Ampere)\n",
    "load_in_4bit = True # ä½¿ç”¨ 4bit é‡åŒ–åŠ è½½\n",
    "\n",
    "# è¿™æ˜¯æ‚¨çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹\n",
    "# ä¾‹å¦‚ï¼š\"qwen-1.5b_lora_model\"\n",
    "# model_name = \"qwen-1.5b_lora_model\" \n",
    "# model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\" \n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\" \n",
    "\n",
    "# è¿™ä¸€æ­¥ä¼šè¿”å›ä¸€ä¸ªç»è¿‡ Unsloth ä¼˜åŒ–çš„æ¨¡å‹å’Œä¸€ä¸ªåˆ†è¯å™¨\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å¾®è°ƒå‰æ¨ç†æµ‹è¯•\n",
    "\n",
    "åœ¨å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç”¨å®ƒæ¥ç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼Œçœ‹çœ‹åŸå§‹æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ã€‚è¿™å¯ä»¥ä½œä¸ºæˆ‘ä»¬å¾®è°ƒæ•ˆæœçš„åŸºå‡†å‚è€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹æ¨ç†çš„ Prompt æ¨¡æ¿\n",
    "inference_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "question = \"ç”·ï¼Œ28å²ï¼Œç¨‹åºå‘˜ï¼Œæœ€è¿‘ä¸€å‘¨æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶å€™è¿˜æ¶å¿ƒã€‚\"\n",
    "\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "attention_mask = inputs.input_ids.ne(tokenizer.pad_token_id).long().to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å—¯ï¼Œæˆ‘ç°åœ¨è¦å¤„ç†è¿™ä½28å²çš„ç¨‹åºå‘˜çš„ç—…ä¾‹ã€‚ä»–æœ€è¿‘å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶è¿˜ä¼šæ¶å¿ƒã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦è€ƒè™‘ä»–çš„å·¥ä½œæƒ…å†µï¼Œå› ä¸ºä»–æ¯å¤©å·¥ä½œåˆ°å¾ˆæ™šï¼Œè¿™å¯èƒ½å¯¹ä»–çš„èº«ä½“æœ‰å½±å“ï¼Œæ¯”å¦‚ç–²åŠ³ã€ç¡çœ ä¸è¶³ï¼Œè¿™äº›éƒ½æ˜¯å¯èƒ½å¯¼è‡´å¤´æ™•å’Œæ¶å¿ƒçš„åŸå› ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘æƒ³åˆ°å¯èƒ½ä»–æœ‰é¢ˆæ¤é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¦‚æœä»–é•¿æ—¶é—´ä½å¤´çœ‹ç”µè„‘ï¼Œå¯èƒ½ä¼šå‹è¿«é¢ˆæ¤ï¼Œå¯¼è‡´ç–¼ç—›å’Œä¸é€‚ã€‚è¿™ç§æƒ…å†µæ¯”è¾ƒå¸¸è§ï¼Œå°¤å…¶æ˜¯å¹´è½»äººé•¿æ—¶é—´ä½¿ç”¨ç”µå­è®¾å¤‡çš„äººç¾¤ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘ä»–çš„ç—‡çŠ¶æ˜¯å¦ç¬¦åˆæŸç§ç‰¹å®šçš„ç–¾ç—…ã€‚å¤´æ™•å’Œæ¶å¿ƒå¯èƒ½ä¸å‰åº­ç³»ç»Ÿæœ‰å…³ï¼Œæ¯”å¦‚æ¢…å°¼åŸƒç—…ï¼Œè¿™æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œå¯èƒ½å¯¼è‡´æ¶å¿ƒã€å‘•åã€å¤´æ™•ç­‰ç—‡çŠ¶ã€‚ä½†æ¢…å°¼åŸƒç—…é€šå¸¸ä¼šä¼´éšè€³éƒ¨ç—‡çŠ¶ï¼Œæ¯”å¦‚å¬åŠ›å‡é€€æˆ–è€³ç—›ï¼Œå¦‚æœä»–æ²¡æœ‰è¿™äº›ç—‡çŠ¶ï¼Œå¯èƒ½æ’é™¤è¿™ä¸ªå¯èƒ½æ€§ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œæˆ‘æƒ³åˆ°ä»–å¯èƒ½æœ‰ç¡çœ éšœç¢ï¼Œæ¯”å¦‚å¤±çœ æˆ–è€…ç¡çœ è´¨é‡ä¸å¥½ï¼Œå¯¼è‡´ç™½å¤©ç²¾åŠ›ä¸è¶³ï¼Œå‡ºç°å¤´æ™•å’Œæ¶å¿ƒã€‚ç¡çœ ä¸è¶³ä¼šå¯¼è‡´å¤§è„‘ä¾›è¡€ä¸è¶³ï¼Œä»è€Œäº§ç”Ÿç–²åŠ³æ„Ÿå’Œä¸é€‚æ„Ÿã€‚\n",
      "\n",
      "æˆ‘è¿˜åº”è¯¥è€ƒè™‘å…¶ä»–å¯èƒ½çš„å¥åº·é—®é¢˜ï¼Œæ¯”å¦‚å‹åŠ›è¿‡å¤§ã€ç„¦è™‘æˆ–è€…æŠ‘éƒï¼Œè¿™äº›å¿ƒç†å› ç´ ä¹Ÿå¯èƒ½å¯¼è‡´ç±»ä¼¼çš„ç—‡çŠ¶ã€‚ä½†é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™äº›ç—‡çŠ¶å¯èƒ½æ›´æŒä¹…ï¼Œè€Œä¸ä»…ä»…æ˜¯å·¥ä½œåˆ°æ·±å¤œå¼•èµ·çš„ã€‚\n",
      "\n",
      "ç„¶åï¼Œæˆ‘éœ€è¦å»ºè®®ä»–åšä¸€äº›æ£€æŸ¥ï¼Œæ¯”å¦‚è¡€å¸¸è§„ã€å¤´éƒ¨å½±åƒå­¦æ£€æŸ¥ï¼ˆå¦‚CTæˆ–MRIï¼‰ã€ç¥ç»åŠŸèƒ½æ£€æŸ¥ï¼Œä»¥åŠè¯„ä¼°ä»–çš„ç¡çœ è´¨é‡ã€‚è¿™äº›æ£€æŸ¥å¯ä»¥å¸®åŠ©ç¡®å®šå…·ä½“çš„åŸå› ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œæˆ‘åº”è¯¥å»ºè®®ä»–è¿›è¡Œä¸€äº›ç¼“è§£ç—‡çŠ¶çš„æ´»åŠ¨ï¼Œæ¯”å¦‚æ”¾æ¾è®­ç»ƒã€æ·±å‘¼å¸æˆ–è€…æ”¹å˜å·¥ä½œå§¿åŠ¿ï¼Œå¦‚æœè¿™äº›æ–¹æ³•æ— æ•ˆï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„åŒ»ç–—å»ºè®®ã€‚\n",
      "\n",
      "æ€»çš„æ¥è¯´ï¼Œæˆ‘éœ€è¦ç»¼åˆè€ƒè™‘ä»–çš„ç—‡çŠ¶ã€ç”Ÿæ´»æ–¹å¼ä»¥åŠå¯èƒ½çš„å¥åº·é—®é¢˜ï¼Œç„¶åç»™å‡ºä¸€ä¸ªå…¨é¢çš„è¯„ä¼°å’Œå»ºè®®ã€‚\n",
      "</think>\n",
      "\n",
      "æ ¹æ®æ‚¨çš„æè¿°ï¼Œè¿™ä½28å²çš„ç¨‹åºå‘˜å‡ºç°å¤´æ™•ã€è„–å­ç–¼å’Œæ¶å¿ƒï¼Œå¯èƒ½ä¸å¤šç§å› ç´ ç›¸å…³ã€‚ä»¥ä¸‹æ˜¯ä¸€ä»½åˆ†æ­¥çš„åˆ†ææŠ¥å‘Šï¼š\n",
      "\n",
      "1. **åˆæ­¥è¯„ä¼°**ï¼š\n",
      "   - **ç—‡çŠ¶ç±»å‹**ï¼šå¤´æ™•ã€è„–å­ç–¼ç—›ã€æ¶å¿ƒã€‚\n",
      "   - **å¯èƒ½åŸå› **ï¼š\n",
      "     - å·¥ä½œå‹åŠ›å’Œç–²åŠ³ã€‚\n",
      "     - é¢ˆéƒ¨å§¿åŠ¿ä¸èˆ’é€‚ã€‚\n",
      "     - å¿ƒè¡€ç®¡æˆ–ç¥ç»ç³»ç»Ÿé—®é¢˜ã€‚\n",
      "     - æ¢…å°¼åŸƒç—…ï¼ˆéœ€è€ƒè™‘æ˜¯å¦æœ‰è€³éƒ¨ç—‡çŠ¶ï¼‰ã€‚\n",
      "     - ç¡çœ éšœç¢ã€‚\n",
      "\n",
      "2. **å»ºè®®æ£€æŸ¥**ï¼š\n",
      "   - **è¡€æ¶²æ£€æŸ¥**ï¼šè¯„ä¼°è´«è¡€æˆ–ä½è¡€ç³–ã€‚\n",
      "   - **å¤´éƒ¨å½±åƒå­¦æ£€æŸ¥**ï¼ˆå¦‚CTæˆ–MRIï¼‰ï¼šè¯„ä¼°é¢ˆæ¤æˆ–å¤´éƒ¨ç»“æ„ã€‚\n",
      "   - **ç¥ç»åŠŸèƒ½è¯„ä¼°**ï¼šè¯„ä¼°ç¥ç»åŠŸèƒ½å®Œæ•´æ€§ã€‚\n",
      "   - **ç¡çœ è´¨é‡è¯„ä¼°**ï¼šè¯„ä¼°æ˜¯å¦å­˜åœ¨å¤±çœ æˆ–ç¡çœ éšœç¢ã€‚\n",
      "\n",
      "3. **åˆæ­¥å»ºè®®**ï¼š\n",
      "   - **ä¼‘æ¯ä¸è°ƒæ•´å·¥ä½œæ–¹å¼**ï¼šå»ºè®®ä»–è°ƒæ•´å·¥ä½œå§¿åŠ¿ï¼Œé¿å…é•¿æ—¶é—´ä½å¤´ã€‚\n",
      "   - **æ”¹å–„ç¡çœ **ï¼šå»ºè®®ä»–å°è¯•æ”¹å–„ç¡çœ è´¨é‡ï¼Œå¦‚å®šæ—¶å…¥çœ ã€‚\n",
      "   - **å¿ƒç†è°ƒèŠ‚**ï¼šå¦‚æœä¼´éšç„¦è™‘æˆ–å‹åŠ›ï¼Œå»ºè®®è¿›è¡Œå¿ƒç†è°ƒèŠ‚ã€‚\n",
      "\n",
      "4. **è¿›ä¸€æ­¥è¡ŒåŠ¨**ï¼š\n",
      "   - **å°±åŒ»æ£€æŸ¥**ï¼šå°½å¿«å®‰æ’ç¥ç»ç§‘æˆ–å†…ç§‘ä¼šè¯Šï¼Œè¿›è¡Œè¯¦ç»†æ£€æŸ¥ã€‚\n",
      "   - **è®°å½•ç—‡çŠ¶**ï¼šè¯¦ç»†è®°å½•ç—‡çŠ¶å˜åŒ–ï¼Œæ–¹ä¾¿åç»­è¯Šæ–­ã€‚\n",
      "\n",
      "5. **æ³¨æ„äº‹é¡¹**ï¼š\n",
      "   - **é¿å…å‰§çƒˆè¿åŠ¨**ï¼šé¿å…å½±å“é¢ˆæ¤ã€‚\n",
      "   - **ä¿æŒæ°´åˆ†æ‘„å…¥**ï¼šå¤šå–æ°´ï¼Œé˜²æ­¢è„±æ°´ã€‚\n",
      "   - **è®°å½•æ—¥å¿—**ï¼šè®°å½•å·¥ä½œæ—¶é—´ã€ç—‡çŠ¶å˜åŒ–ï¼Œæ–¹ä¾¿åŒ»ç”Ÿè¯„ä¼°ã€‚\n",
      "\n",
      "é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œå¯ä»¥æ›´å…¨é¢åœ°è¯„ä¼°å’Œå¤„ç†æ‚¨çš„ç—‡çŠ¶ã€‚å»ºè®®åœ¨åŒ»ç”ŸæŒ‡å¯¼ä¸‹è¿›è¡Œè¯¦ç»†æ£€æŸ¥å’Œæ²»ç–—ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. ä¸‹è½½å’Œæ ¼å¼åŒ–è®­ç»ƒæ•°æ®é›†\n",
    "\n",
    "\n",
    "åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT/viewer/zh\n",
    "\n",
    "![dataset](images/dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è®­ç»ƒçš„ Prompt æ¨¡æ¿\n",
    "train_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # æ·»åŠ  EOS Token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        # å°† EOS Token æ·»åŠ åˆ°æ ·æœ¬æœ€å\n",
    "        text = train_prompt.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"zh\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\\nè¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\\nåœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\\n\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\\nè¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\\n\\n### Question:\\næ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\\n\\n### Response:\\n<think>\\nè¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\\n\\nç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\\n\\nä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\\n\\nç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\\n\\nä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\\n\\nå›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\\n\\nå†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\\n\\nçœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\\n\\nä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\\n\\nå“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\\n</think>\\nä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\\n<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
       "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
       "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
       "\n",
       "\n",
       "### Instruction:\n",
       "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
       "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
       "\n",
       "### Question:\n",
       "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\n",
       "\n",
       "### Response:\n",
       "<think>\n",
       "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
       "\n",
       "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
       "\n",
       "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
       "\n",
       "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
       "\n",
       "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
       "\n",
       "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
       "\n",
       "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
       "\n",
       "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
       "\n",
       "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
       "\n",
       "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\n",
       "</think>\n",
       "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
       "<ï½œendâ–ofâ–sentenceï½œ>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(dataset[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ä½¿ç”¨ Unsloth æ·»åŠ  LoRA é€‚é…å™¨\n",
    "\n",
    "è¿™æ˜¯ä½¿ç”¨ `unsloth` çš„æ ¸å¿ƒæ­¥éª¤ã€‚æˆ‘ä»¬è°ƒç”¨ `FastLanguageModel.get_peft_model`ï¼Œå®ƒä¼šéå¸¸é«˜æ•ˆåœ°ä¸ºæ¨¡å‹æ³¨å…¥ LoRA æ¨¡å—ã€‚\n",
    "\n",
    "- `r`: LoRA çš„ç§© (rank)ï¼Œæ˜¯æ§åˆ¶æ¨¡å‹å¤æ‚åº¦å’Œå‚æ•°é‡çš„å…³é”®è¶…å‚æ•°ã€‚\n",
    "- `target_modules`: æŒ‡å®šè¦åœ¨å“ªäº›çº¿æ€§å±‚ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶çš„ q, k, v, o æŠ•å½±å±‚ï¼‰ä¸Šåº”ç”¨ LoRAã€‚\n",
    "- `lora_alpha`: LoRA çš„ç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ç½®ä¸º `r` çš„ä¸¤å€æˆ–ä¸ `r` ç›¸åŒã€‚\n",
    "- `use_gradient_checkpointing`: ä¸€ç§èŠ‚çœæ˜¾å­˜çš„æŠ€æœ¯ï¼Œå¯¹äºè®­ç»ƒå¤§æ¨¡å‹è‡³å…³é‡è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "          (4): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "          (5-10): 6 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "          (11): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "          (12-23): 12 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "          (24-26): 3 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "          (27): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸º `model` å¯¹è±¡ç°åœ¨æ˜¯ç”± Unsloth åˆ›å»ºçš„ï¼Œå®ƒåŒ…å«äº†æ‰€æœ‰å¿…éœ€çš„å±æ€§\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "      \"q_proj\",\n",
    "      \"k_proj\",\n",
    "      \"v_proj\",\n",
    "      \"o_proj\",\n",
    "      \"gate_proj\",\n",
    "      \"up_proj\",\n",
    "      \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=1432,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "# æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼Œç¡®è®¤ LoRA é€‚é…å™¨å·²æ·»åŠ \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. é…ç½® SFTTrainer\n",
    "\n",
    "`SFTTrainer` (Supervised Fine-tuning Trainer) æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæŒ‡ä»¤å¾®è°ƒçš„è®­ç»ƒå™¨ã€‚æˆ‘ä»¬éœ€è¦é…ç½® `TrainingArguments` æ¥æŒ‡å®šæ‰€æœ‰çš„è®­ç»ƒå‚æ•°ï¼Œå¦‚æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4c52c8e90644549d0eac251fbfe2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/20171 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 64,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 80,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1432,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè°ƒç”¨ `trainer.train()` å³å¯å¼€å§‹å¾®è°ƒè¿‡ç¨‹ã€‚è®­ç»ƒç»“æŸåï¼Œä¼šè¿”å›åŒ…å«è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚è®­ç»ƒæŸå¤±ï¼‰çš„å¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 2 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 57 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (57 x 2 x 1) = 114\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 51 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (51 x 2 x 1) = 102\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 45 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (45 x 2 x 1) = 90\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 40 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (40 x 2 x 1) = 80\n",
      " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 33:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.982800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.487400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.145200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.089600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.056500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.983200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.991800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.962400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.948000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.939700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.999600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.917300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.950600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.920300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.977500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.913000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>1.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.994200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.927100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.960100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.943700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.921300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.919200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.919400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.957000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.922100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.946200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=80, training_loss=2.112584951519966, metrics={'train_runtime': 2059.2829, 'train_samples_per_second': 3.108, 'train_steps_per_second': 0.039, 'total_flos': 2.6008130525110272e+17, 'train_loss': 2.112584951519966})\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\n",
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ï¼ˆLoraï¼‰\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥å†æ¬¡è¿›è¡Œæ¨ç†ï¼Œæ¯”è¾ƒå¾®è°ƒåçš„æ¨¡å‹ä¸åŸå§‹æ¨¡å‹çš„å·®å¼‚ã€‚å¦‚æœå¯¹ç»“æœæ»¡æ„ï¼Œå¯ä»¥ä½¿ç”¨ `model.save_pretrained(\"your_lora_adapter_path\")` æ¥ä¿å­˜è®­ç»ƒå¥½çš„ LoRA é€‚é…å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qwen-7b_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen-7b_lora_model/tokenizer_config.json',\n",
       " 'qwen-7b_lora_model/special_tokens_map.json',\n",
       " 'qwen-7b_lora_model/chat_template.jinja',\n",
       " 'qwen-7b_lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"qwen-7b_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ä¿å­˜æ–¹å¼äºŒé€‰ä¸€ï¼ˆè¦ä¹ˆä½¿ç”¨ä¸Šé¢çš„åˆ†å¼€ä¿å­˜ï¼Œè¦ä¹ˆä½¿ç”¨è¿™é‡Œçš„åˆå¹¶ Lora ä¿å­˜ï¼‰\n",
    "# model.save_pretrained_merged(\"qwen-1.5b_lora_model\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. æµ‹è¯•è®­ç»ƒåçš„ç”Ÿæˆç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "question=\"ä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”å¦‚ä½•å¤„ç†ï¼Ÿ\", # Question\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1000,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "è¿™ä½ç—…äººå·²ç»æœ‰5å¤©çš„æ€¥æ€§é˜‘å°¾ç‚äº†ï¼Œè…¹ç—›è™½ç„¶ç¨å¾®å‡è½»äº†ï¼Œä½†è¿˜æ˜¯æœ‰ç‚¹ç–¼ç—›ã€‚è€Œä¸”å‘çƒ­ä¹Ÿæ˜¯ä¸ªé—®é¢˜ã€‚ç°åœ¨ä½“æ£€çš„æ—¶å€™ï¼Œå³ä¸‹è…¹æœ‰ä¸€ä¸ªå‹ç—›çš„åŒ…å—ã€‚å—¯ï¼Œè¿™è¯´æ˜å¯èƒ½æœ‰æ„ŸæŸ“äº†ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘å¾—æƒ³åˆ°é˜‘å°¾ç‚é€šå¸¸ä¼šä¼´éšæ„ŸæŸ“ï¼Œæ‰€ä»¥è¿™ä¸ªåŒ…å—å¯èƒ½å°±æ˜¯æ„ŸæŸ“çš„è¿¹è±¡ã€‚ä¸è¿‡ï¼ŒåŒ…å—å¯èƒ½ä¸æ˜¯é˜‘å°¾ç‚çš„å”¯ä¸€è¡¨ç°ï¼Œä¹Ÿå¯èƒ½æ˜¯å…¶ä»–é—®é¢˜ï¼Œæ¯”å¦‚æ€¥æ€§èƒ†å›Šç‚æˆ–è€…æ€¥æ€§èƒƒç‚ã€‚\n",
      "\n",
      "å“¦ï¼Œæ€¥æ€§é˜‘å°¾ç‚å¸¸ä¼´æœ‰å³ä¸‹è…¹çš„å‹ç—›ï¼Œè¿™å¯èƒ½æ˜¯åŒ…å—çš„ä¿¡å·ã€‚ä½†æ˜¯ï¼Œå¦‚æœåŒ…å—æœ‰å‹ç—›ï¼Œè€Œä¸”å‘çƒ­çš„è¯ï¼Œé‚£æƒ…å†µå°±ä¸ä¸€æ ·äº†ã€‚å‘çƒ­ä¸€èˆ¬å’Œæ„ŸæŸ“æœ‰å…³ï¼Œæ¯”å¦‚ç»†èŒæ„ŸæŸ“ï¼Œè¿™è®©æˆ‘è§‰å¾—å¯èƒ½ä¸æ˜¯é˜‘å°¾ç‚ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œé˜‘å°¾ç‚ä¸€èˆ¬ä¸ä¼šæœ‰å‘çƒ­ï¼Œé™¤éæ˜¯ç»†èŒæ„ŸæŸ“ï¼Œä½†è¿™ç§æƒ…å†µä¸‹ï¼ŒåŒ…å—å’Œå‘çƒ­éƒ½ä¼šå‡ºç°ã€‚é‚£è¿™ä¸ªåŒ…å—æ˜¯ä¸æ˜¯ä»£è¡¨æ„ŸæŸ“äº†æŸç§ç»†èŒå‘¢ï¼Ÿ\n",
      "\n",
      "æŸ¥ä¸€ä¸‹ï¼Œå¦‚æœåŒ…å—æ˜¯æ„ŸæŸ“æ€§çš„ï¼Œé‚£æˆ‘ä»¬éœ€è¦è€ƒè™‘å¯èƒ½çš„æ„ŸæŸ“åŸå› ï¼Œæ¯”å¦‚èƒ†å›Šç‚æˆ–è€…æ€¥æ€§èƒƒç‚ï¼Œè¿™äº›éƒ½ä¼šå¯¼è‡´å‘çƒ­å’ŒåŒ…å—ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œåˆ«æ€¥ç€ä¸‹ç»“è®ºï¼Œå¾—å†ä»”ç»†æƒ³æƒ³ã€‚æ€¥æ€§é˜‘å°¾ç‚ä¸€èˆ¬ä¸ä¼šä¼´éšå‘çƒ­ï¼Œé™¤éæ˜¯ç»†èŒæ„ŸæŸ“ã€‚é‚£è¿™ä¸ªåŒ…å—å’Œå‘çƒ­å¯èƒ½æŒ‡å‘ç»†èŒæ„ŸæŸ“ï¼Œè¿™è®©æˆ‘æ€€ç–‘æ˜¯æ€¥æ€§èƒ†å›Šç‚ã€‚\n",
      "\n",
      "å—¯ï¼Œæˆ‘éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤ï¼Œä½†æ ¹æ®ç›®å‰çš„ä¿¡æ¯ï¼Œè¿™ä¸ªåŒ…å—å’Œå‘çƒ­æç¤ºå¯èƒ½æ˜¯ç»†èŒæ„ŸæŸ“å¼•èµ·çš„æ€¥æ€§èƒ†å›Šç‚ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯å…¶ä»–åŸå› ï¼Œæ¯”å¦‚æ€¥æ€§èƒƒç‚ï¼Œè™½ç„¶é€šå¸¸ä¸ä¼´éšå‘çƒ­ï¼Œä½†æœ‰æ—¶å€™ä¹Ÿä¼šæœ‰ï¼Œæ‰€ä»¥ä¹Ÿä¸èƒ½æ’é™¤ã€‚\n",
      "\n",
      "å•Šï¼Œè¿™ä¸ªåŒ…å—çš„å‹ç—›å’Œå‘çƒ­ï¼Œè®©æˆ‘æ›´å€¾å‘äºç»†èŒæ„ŸæŸ“çš„å¯èƒ½æ€§ï¼Œå°¤å…¶æ˜¯æ€¥æ€§èƒ†å›Šç‚ã€‚\n",
      "\n",
      "å¥½å§ï¼Œçœ‹æ¥æˆ‘å¾—æŒ‰ç…§è¿™ä¸ªæ€è·¯æ¥å¤„ç†ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ï¼Œæ¯”å¦‚Bè¶…æˆ–è€…åšä¸€ä¸ªå½±åƒå­¦æ£€æŸ¥ï¼Œçœ‹çœ‹åŒ…å—çš„å…·ä½“æƒ…å†µã€‚\n",
      "\n",
      "æ€»ä¹‹ï¼Œè¿™ä¸ªåŒ…å—å’Œå‘çƒ­æç¤ºå¯èƒ½æ˜¯ç»†èŒæ„ŸæŸ“å¼•èµ·çš„æ€¥æ€§èƒ†å›Šç‚ï¼Œæˆ‘å¾—è®°ä½è¿™ä¸ªæƒ…å†µï¼Œä»¥ä¾¿é‡‡å–ç›¸åº”çš„æ²»ç–—æªæ–½ã€‚\n",
      "</think>\n",
      "æ ¹æ®æ‚¨çš„æè¿°ï¼Œè¿™ä¸ªç—…äººæœ‰5å¤©çš„æ€¥æ€§é˜‘å°¾ç‚ç—…å²ï¼Œè…¹ç—›è™½ç„¶ç¨æœ‰å‡è½»ï¼Œä½†ä»ç„¶å­˜åœ¨å‘çƒ­å’Œå³ä¸‹è…¹çš„å‹ç—›åŒ…å—ã€‚è¿™äº›ç—‡çŠ¶æç¤ºå¯èƒ½å­˜åœ¨ç»†èŒæ„ŸæŸ“ï¼Œå°¤å…¶æ˜¯æ€¥æ€§èƒ†å›Šç‚çš„å¯èƒ½æ€§è¾ƒå¤§ã€‚æ€¥æ€§é˜‘å°¾ç‚é€šå¸¸ä¸ä¼šä¼´éšå‘çƒ­ï¼Œé™¤éæ˜¯ç»†èŒæ„ŸæŸ“ï¼Œè€Œå‘çƒ­å’ŒåŒ…å—çš„å‹ç—›æ›´å¸¸ä¸æ€¥æ€§èƒ†å›Šç‚ç›¸å…³è”ã€‚å› æ­¤ï¼Œå»ºè®®è¿›è¡Œè¿›ä¸€æ­¥çš„å½±åƒå­¦æ£€æŸ¥ï¼Œå¦‚Bè¶…ï¼Œä»¥æ˜ç¡®åŒ…å—çš„æ€§è´¨å’Œä½ç½®ã€‚å¦‚æœåŒ…å—å‘ˆæ„ŸæŸ“æ€§è¡¨ç°ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘æ€¥æ€§èƒ†å›Šç‚çš„è¯Šæ–­ã€‚å»ºè®®æ ¹æ®å½±åƒå­¦ç»“æœå’Œç—‡çŠ¶å˜åŒ–é‡‡å–ç›¸åº”çš„æ²»ç–—æªæ–½ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(output[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str, model, tokenizer, inference_prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œåˆ†è¯å™¨ä¸ºç»™å®šçš„åŒ»å­¦é—®é¢˜ç”Ÿæˆå“åº”ã€‚\n",
    "\n",
    "    Args:\n",
    "        question (str): éœ€è¦æ¨¡å‹å›ç­”çš„åŒ»å­¦é—®é¢˜ã€‚\n",
    "        model: å·²åŠ è½½çš„ Unsloth/Hugging Face æ¨¡å‹ã€‚\n",
    "        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨ã€‚\n",
    "        inference_prompt (str): ç”¨äºæ ¼å¼åŒ–è¾“å…¥çš„ f-string æ¨¡æ¿ã€‚\n",
    "        max_new_tokens (int, optional): ç”Ÿæˆå“åº”çš„æœ€å¤§ token æ•°é‡ã€‚é»˜è®¤ä¸º 1024ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼Œå·²å»é™¤ prompt éƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥\n",
    "    prompt = inference_prompt.format(\n",
    "        question, # å¡«å……é—®é¢˜\n",
    "        \"\",       # ç•™ç©ºï¼Œè®©æ¨¡å‹ç”Ÿæˆ CoT å’Œ Response\n",
    "    )\n",
    "\n",
    "    # 2. å°†æ ¼å¼åŒ–åçš„ prompt è¿›è¡Œåˆ†è¯ï¼Œå¹¶è½¬ç§»åˆ° GPU\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 3. ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡º\n",
    "    # use_cache=True ç”¨äºåŠ é€Ÿè§£ç è¿‡ç¨‹\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    # 4. å°†ç”Ÿæˆçš„ token è§£ç ä¸ºæ–‡æœ¬\n",
    "    # skip_special_tokens=True ä¼šç§»é™¤åƒ EOS_TOKEN è¿™æ ·çš„ç‰¹æ®Šæ ‡è®°\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # 5. åˆ‡åˆ†å­—ç¬¦ä¸²ï¼Œåªè¿”å› \"### Response:\" ä¹‹åçš„éƒ¨åˆ†\n",
    "    # ä½¿ç”¨ .split() åˆ†å‰²å¹¶è·å–å“åº”å†…å®¹ï¼Œ.strip() ç”¨äºå»é™¤å¯èƒ½å­˜åœ¨çš„å‰åç©ºç™½å­—ç¬¦\n",
    "    response_part = decoded_output.split(\"### Response:\")\n",
    "    if len(response_part) > 1:\n",
    "        return response_part[1].strip()\n",
    "    else:\n",
    "        # å¦‚æœæ¨¡å‹æ²¡æœ‰ç”Ÿæˆ \"### Response:\" æ ‡è®°ï¼Œåˆ™è¿”å›æ•´ä¸ªç”Ÿæˆå†…å®¹ä»¥ä¾›è°ƒè¯•\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "60å²ç”·æ€§æ‚£è€…ï¼Œå³èƒ¸ç–¼ï¼ŒXçº¿æ˜¾ç¤ºå³è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œæç¤ºå¯èƒ½æœ‰èƒ¸è…”ç§¯æ¶²ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œæˆ‘å¾—è€ƒè™‘ä¸€ä¸‹èƒ¸æ°´çš„æ€§è´¨ã€‚èƒ¸æ°´çš„æ€§è´¨å¯èƒ½å½±å“è¯Šæ–­å’Œæ²»ç–—ï¼Œæ¯”å¦‚æ˜¯å¦æ˜¯è„“æ¶²è¿˜æ˜¯æ¸…æ¶²ï¼Œæˆ–è€…æœ‰å…¶ä»–æˆåˆ†ã€‚é€šå¸¸ï¼Œäº†è§£èƒ¸æ°´çš„æˆåˆ†å¯¹äºæ²»ç–—å’Œè¯Šæ–­éå¸¸å…³é”®ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘æƒ³åˆ°å¸¸è§„çš„å®éªŒå®¤æ£€æŸ¥ï¼Œæ¯”å¦‚è¡€å¸¸è§„å’Œè‚è‚¾åŠŸèƒ½æ£€æŸ¥ã€‚è¿™äº›æ£€æŸ¥å¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£æ‚£è€…çš„æ€»ä½“å¥åº·çŠ¶å†µï¼Œä»¥åŠæ˜¯å¦å­˜åœ¨æ„ŸæŸ“ã€ç‚ç—‡æˆ–è€…å…¶ä»–ä»£è°¢é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå®ƒä»¬å¯èƒ½æ— æ³•ç›´æ¥æä¾›å…³äºèƒ¸æ°´çš„è¯¦ç»†ä¿¡æ¯ã€‚\n",
      "\n",
      "å—¯ï¼Œå¦‚æœæˆ‘æƒ³æ›´æ·±å…¥åœ°äº†è§£èƒ¸æ°´çš„æ€§è´¨ï¼Œå¯èƒ½éœ€è¦æ›´ç›´æ¥çš„åˆ†æã€‚å®éªŒå®¤æ£€æŸ¥ä¸­ï¼Œè¡€åŸ¹å…»å’Œèƒ¸æ°´åŸ¹å…»æ˜¯å¸¸ç”¨çš„æ‰‹æ®µã€‚è¡€åŸ¹å…»å¯ä»¥æ£€æµ‹æ˜¯å¦å­˜åœ¨ç‰¹å®šç—…åŸä½“ï¼Œæ¯”å¦‚è‚ºç‚é“¾çƒèŒï¼Œè€Œèƒ¸æ°´åŸ¹å…»åˆ™å¯ä»¥åˆ†æèƒ¸æ°´ä¸­çš„ç—…åŸä½“ï¼Œå¦‚ç»†èŒã€ç—…æ¯’æˆ–è€…çœŸèŒã€‚è¿™äº›ä¿¡æ¯å¯¹äºåˆ¤æ–­èƒ¸æ°´çš„æ€§è´¨å’Œè¿›è¡Œé’ˆå¯¹æ€§æ²»ç–—éå¸¸é‡è¦ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œè¿˜æœ‰æ²¡æœ‰å…¶ä»–æ›´ç›´æ¥çš„æ–¹æ³•å‘¢ï¼Ÿæ¯”å¦‚å½±åƒå­¦æ£€æŸ¥ï¼Œæ¯”å¦‚è¶…å£°ï¼Œå¯ä»¥æä¾›èƒ¸æ°´çš„å¤–è§‚ä¿¡æ¯ã€‚ä½†å½±åƒå­¦æ£€æŸ¥åªèƒ½å¤§è‡´äº†è§£èƒ¸æ°´çš„æ¶²é‡å’Œå¤–è§‚ï¼Œä¸èƒ½æä¾›å…·ä½“çš„æˆåˆ†åˆ†æã€‚\n",
      "\n",
      "å“¦ï¼Œå¯¹äº†ï¼Œè¿˜æœ‰ä¸€ä¸ªåŠæ³•ï¼Œé‚£å°±æ˜¯èƒ¸æ°´ç©¿åˆºï¼Œè·å–èƒ¸æ°´æ ·æœ¬è¿›è¡Œè¿›ä¸€æ­¥çš„åˆ†æã€‚é€šè¿‡èƒ¸æ°´ç©¿åˆºï¼Œå¯ä»¥å°†èƒ¸æ°´æ ·æœ¬é€è‡³å®éªŒå®¤è¿›è¡Œå„ç§åˆ†æï¼Œå¦‚åŸ¹å…»ã€åŸºå› æ£€æµ‹ã€ç—…æ¯’æ£€æµ‹ç­‰ã€‚è¿™äº›æ£€æµ‹å¯ä»¥æä¾›å…³äºèƒ¸æ°´çš„æˆåˆ†å’Œç—…åŸä½“çš„ä¿¡æ¯ï¼Œè¿™å¯¹è¯Šæ–­å’Œæ²»ç–—éå¸¸æœ‰å¸®åŠ©ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œèƒ¸æ°´ç©¿åˆºå¯èƒ½åœ¨ä¸´åºŠä¸Šä¸å¤ªå¸¸è§ï¼Œå› ä¸ºæ“ä½œç›¸å¯¹å¤æ‚ï¼Œæˆæœ¬ä¹Ÿè¾ƒé«˜ã€‚ä¸è¿‡ï¼Œå¯¹äºäº†è§£èƒ¸æ°´çš„æ€§è´¨ï¼Œè¿™æ˜¯æœ€ç›´æ¥æœ‰æ•ˆçš„æ–¹æ³•ä¹‹ä¸€ã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œä¸ºäº†æ›´å‡†ç¡®äº†è§£èƒ¸æ°´çš„æˆåˆ†å’Œæ€§è´¨ï¼Œè¡€åŸ¹å…»å’Œèƒ¸æ°´åŸ¹å…»æ˜¯æœ€ç›´æ¥çš„å®éªŒå®¤æ£€æŸ¥æ‰‹æ®µã€‚æ­¤å¤–ï¼Œèƒ¸æ°´ç©¿åˆºä¹Ÿæ˜¯æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå¯ä»¥æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚\n",
      "</think>\n",
      "å¯¹äºä¸€å66å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼å¹¶åœ¨Xçº¿æ£€æŸ¥ä¸­æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œäº†è§£èƒ¸æ°´çš„æ€§è´¨éœ€è¦è¿›è¡Œæ›´æ·±å…¥çš„å®éªŒå®¤æ£€æŸ¥ã€‚ä»¥ä¸‹æ˜¯å¯èƒ½å¯¹äº†è§£èƒ¸æ°´æ€§è´¨æœ‰å¸®åŠ©çš„å®éªŒå®¤æ£€æŸ¥ï¼š\n",
      "\n",
      "1. **è¡€åŸ¹å…»**ï¼šé€šè¿‡è¡€åŸ¹å…»å¯ä»¥æ£€æµ‹æ˜¯å¦å­˜åœ¨ç‰¹å®šç—…åŸä½“ï¼Œå¦‚è‚ºç‚é“¾çƒèŒã€ç»“æ ¸èŒç­‰ï¼Œè¿™æœ‰åŠ©äºäº†è§£èƒ¸æ°´ä¸­çš„ç—…åŸä½“ç§ç±»ã€‚\n",
      "\n",
      "2. **èƒ¸æ°´åŸ¹å…»**ï¼šèƒ¸æ°´åŸ¹å…»æ˜¯ç›´æ¥åˆ†æèƒ¸æ°´æˆåˆ†å’Œç—…åŸä½“çš„å¸¸ç”¨æ–¹æ³•ã€‚é€šè¿‡åŸ¹å…»å¯ä»¥ç¡®å®šèƒ¸æ°´ä¸­çš„ç—…åŸä½“ç±»å‹ï¼Œå¦‚ç»†èŒã€ç—…æ¯’æˆ–çœŸèŒï¼Œä»è€Œæ˜ç¡®èƒ¸æ°´çš„æ€§è´¨ã€‚\n",
      "\n",
      "3. **èƒ¸æ°´ç©¿åˆº**ï¼šèƒ¸æ°´ç©¿åˆºå¯ä»¥è·å–èƒ¸æ°´æ ·æœ¬ï¼Œè¿›ä¸€æ­¥è¿›è¡ŒåŸºå› æ£€æµ‹ã€ç—…æ¯’æ£€æµ‹ç­‰ï¼Œå¸®åŠ©äº†è§£èƒ¸æ°´çš„æˆåˆ†å’Œç—…åŸä½“çš„ç§ç±»ã€‚\n",
      "\n",
      "è¿™äº›æ£€æŸ¥æ–¹æ³•èƒ½å¤Ÿæä¾›å…³äºèƒ¸æ°´æˆåˆ†å’Œç—…åŸä½“çš„è¯¦ç»†ä¿¡æ¯ï¼Œæœ‰åŠ©äºè¯Šæ–­å’Œæ²»ç–—è®¡åˆ’çš„åˆ¶å®šã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å60å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼å¹¶åœ¨Xçº¿æ£€æŸ¥ä¸­æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œè¯·é—®å“ªä¸€é¡¹å®éªŒå®¤æ£€æŸ¥å¯¹äº†è§£èƒ¸æ°´çš„æ€§è´¨æ›´æœ‰å¸®åŠ©ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä½28å²çš„ç¨‹åºå‘˜ï¼Œé•¿æœŸç†¬å¤œï¼Œæœ€è¿‘çªç„¶è§‰å¾—å¤´æ™•ç›®çœ©ï¼Œæœ‰ç‚¹æ¶å¿ƒï¼Œè¿™è®©æˆ‘æƒ³åˆ°ä»–å¯èƒ½æœ‰é«˜è¡€å‹ã€‚ä¸è¿‡ï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯å…¶ä»–ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘å¾—è€ƒè™‘ä»–æœ€è¿‘çš„ç”Ÿæ´»ä¹ æƒ¯ã€‚é•¿æœŸç†¬å¤œï¼Œå¯èƒ½å½±å“äº†ä»–çš„è¡€å‹ã€‚é•¿æ—¶é—´å¤„äºé»‘æš—å’Œä½å…‰ç¯å¢ƒä¸‹ï¼Œè¿™ä¼šé™ä½èº«ä½“å¯¹å…‰çš„æ•æ„Ÿåº¦ï¼Œä»è€Œé™ä½è§†ç½‘è†œä¸­çš„è§†æ†çŠ¶ç»†èƒçš„å…´å¥‹æ€§ï¼Œè¿™å¯èƒ½å¼•èµ·è§†æ†çŠ¶ç»†èƒå‡å°‘ï¼Œè¿›è€Œå¯¼è‡´è¡€å‹å‡é«˜ã€‚\n",
      "\n",
      "ä½†æ˜¯ï¼Œé«˜è¡€å‹ä¹Ÿä¼šä¼´éšä¸€äº›ç—‡çŠ¶ï¼Œæ¯”å¦‚å£æ¸´ã€å¤šå°¿ã€å¤šé¥®å’Œè§†ç‰©æ¨¡ç³Šã€‚è¿™ä½æ‚£è€…æ²¡æœ‰æåˆ°è¿™äº›ç—‡çŠ¶ï¼Œæ‰€ä»¥å¯èƒ½ä¸æ˜¯é«˜è¡€å‹ã€‚\n",
      "\n",
      "é‚£ï¼Œä»–æœ€è¿‘æœ‰æ²¡æœ‰å…¶ä»–ç—‡çŠ¶å‘¢ï¼Ÿæ¯”å¦‚å¤´ç—›ã€å£æ¸´æˆ–è§†ç‰©æ¨¡ç³Šï¼Ÿå¦‚æœä»–æ²¡æœ‰è¿™äº›ç—‡çŠ¶ï¼Œé‚£å¯èƒ½ä¸æ˜¯é«˜è¡€å‹ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œä»–é•¿æœŸç†¬å¤œï¼Œå¯èƒ½è¿˜æœ‰å…¶ä»–åŸå› ã€‚æ¯”å¦‚ï¼Œä»–å¯èƒ½åœ¨ç¡çœ è´¨é‡ä¸Šå‡ºäº†é—®é¢˜ã€‚ç¡çœ ä¸è¶³ï¼Œä¼šå¯¼è‡´èº«ä½“çš„è¡€å‹å‡é«˜ã€‚\n",
      "\n",
      "ä¹Ÿæœ‰å¯èƒ½æ˜¯ä»–çš„å·¥ä½œå‹åŠ›å¤§ï¼Œå¯¼è‡´å‹åŠ›æ€§é«˜è¡€å‹ã€‚å‹åŠ›ç¡®å®ä¼šå¯¼è‡´è¡€å‹å‡é«˜ï¼Œå°¤å…¶æ˜¯é•¿æ—¶é—´å¤„äºç´§å¼ çŠ¶æ€ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œæ‚£è€…æ²¡æœ‰æåˆ°å‹åŠ›æ€§é«˜è¡€å‹çš„ç—‡çŠ¶ï¼Œæ¯”å¦‚å£æ¸´ã€å¤šå°¿ã€è§†ç‰©æ¨¡ç³Šï¼Œæ‰€ä»¥å¯èƒ½ä¸æ˜¯å‹åŠ›æ€§é«˜è¡€å‹ã€‚\n",
      "\n",
      "è¿˜æœ‰ï¼Œä»–æœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè¡€å‹ä¸Šå‡å¼•èµ·çš„è„‘ä¾›è¡€ä¸è¶³ã€‚ä½†æ˜¯ï¼Œå¦‚æœæ²¡æœ‰å£æ¸´ã€å¤šå°¿å’Œè§†ç‰©æ¨¡ç³Šç­‰ç—‡çŠ¶ï¼Œå¯èƒ½ä¸æ˜¯é«˜è¡€å‹ã€‚\n",
      "\n",
      "å†æƒ³æƒ³ï¼Œä»–æœ€è¿‘å¯èƒ½æœ‰å…¶ä»–ç–¾ç—…ã€‚æ¯”å¦‚ï¼Œä»–å¯èƒ½æœ‰ç³–å°¿ç—…ï¼Œæˆ–è€…ä»–å¯èƒ½æœ‰ç¡çœ éšœç¢ï¼Œæ¯”å¦‚å¤±çœ æˆ–è€…ç¡çœ å‘¼å¸æš‚åœã€‚\n",
      "\n",
      "ä¹Ÿæœ‰å¯èƒ½æ˜¯å…¶ä»–åŸå› ï¼Œæ¯”å¦‚ä»–æœ€è¿‘å¼€å§‹åƒæŸäº›è¯ç‰©ï¼Œæˆ–è€…ä»–æœ‰æŸäº›ç”Ÿæ´»ä¹ æƒ¯å¯¼è‡´è¡€å‹å¼‚å¸¸ã€‚\n",
      "\n",
      "å—¯ï¼Œçœ‹æ¥æˆ‘å¾—è€ƒè™‘æ›´å¤šçš„å¯èƒ½æ€§ï¼Œæ¯”å¦‚ç¡çœ éšœç¢ã€å‹åŠ›æ€§é«˜è¡€å‹æˆ–è€…å…¶ä»–æ…¢æ€§ç—…ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œä»ç›®å‰çš„ç—‡çŠ¶æ¥çœ‹ï¼Œä»–å¯èƒ½æœ‰é«˜è¡€å‹ï¼Œä½†ä¹Ÿéœ€è¦æ’é™¤å…¶ä»–å¯èƒ½æ€§ã€‚\n",
      "\n",
      "æˆ–è€…ï¼Œä»–çš„å¤´æ™•ç›®çœ©å¯èƒ½æ˜¯ç”±äºå…¶ä»–åŸå› å¼•èµ·çš„ï¼Œæ¯”å¦‚è„‘ä¾›è¡€ä¸è¶³ã€è´«è¡€ã€è„‘æ°´è‚¿ç­‰ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œæ‚£è€…æ²¡æœ‰æåˆ°è¿™äº›ç—‡çŠ¶ï¼Œæ‰€ä»¥å¯èƒ½è¿˜æ˜¯é«˜è¡€å‹çš„å¯èƒ½æ€§æ›´å¤§ã€‚\n",
      "\n",
      "å—¯ï¼Œç»¼åˆæ¥çœ‹ï¼Œè™½ç„¶ä»–æ²¡æœ‰æåˆ°ä¸€äº›é«˜è¡€å‹çš„ç—‡çŠ¶ï¼Œä½†é•¿æœŸç†¬å¤œå¯èƒ½å½±å“è¡€å‹ï¼Œå¯¼è‡´å¤´æ™•ç›®çœ©ã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œæˆ‘è¿˜æ˜¯å€¾å‘äºè®¤ä¸ºä»–å¯èƒ½æœ‰é«˜è¡€å‹ï¼Œè™½ç„¶éœ€è¦è¿›ä¸€æ­¥çš„æ£€æŸ¥æ¥ç¡®è®¤ã€‚\n",
      "</think>\n",
      "æ ¹æ®è¿™ä½28å²ç¨‹åºå‘˜çš„æè¿°ï¼Œä»–é•¿æœŸç†¬å¤œä¸”æœ€è¿‘çªç„¶å‡ºç°å¤´æ™•ç›®çœ©ï¼Œè¿™ç§æƒ…å†µå¯èƒ½ä¸é«˜è¡€å‹æœ‰å…³ã€‚é•¿æœŸç†¬å¤œå¯èƒ½å¯¼è‡´è§†æ†çŠ¶ç»†èƒå‡å°‘ï¼Œè¿›è€Œå¼•å‘è¡€å‹å‡é«˜ã€‚ç„¶è€Œï¼Œæ‚£è€…æ²¡æœ‰æåˆ°å£æ¸´ã€å¤šå°¿ã€è§†ç‰©æ¨¡ç³Šç­‰é«˜è¡€å‹çš„å…¸å‹ç—‡çŠ¶ï¼Œè¿™å¯èƒ½éœ€è¦è¿›ä¸€æ­¥ç¡®è®¤ã€‚å»ºè®®è¿›è¡Œè¯¦ç»†çš„ä½“æ£€å’Œè¡€å‹ç›‘æµ‹ï¼Œä»¥æ˜ç¡®è¯Šæ–­ã€‚å¦‚æœè¡€å‹å¼‚å¸¸ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥çš„æ²»ç–—ï¼Œä»¥æ’é™¤å…¶ä»–æ½œåœ¨çš„å¥åº·é—®é¢˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å 28 å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œå¸¸å¹´ç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒã€‚è¯·é—®æœ‰å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
